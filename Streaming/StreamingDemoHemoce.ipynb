{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "name": "StreamingDemoHemoce.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sukYe_IhLBOL",
        "colab_type": "text"
      },
      "source": [
        "### Demo task\n",
        "\n",
        "Given a sequence of numbers ($0, 1, 2, ... N$) arriving in real time. You need to calculate the sum of this sequence.\n",
        "Use sateful approach to store and update the current result.\n",
        "\n",
        "The output should be one number.\n",
        "\n",
        "**Example**\n",
        "* Input sequence: `0, 1, 2, 3`.\n",
        "* Output: `6`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RggOdYsQOA0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8b9ffd8c-7b94-4080-ceb7-97c1c666073e"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-3.0.0-preview/spark-3.0.0-preview-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.0-preview-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-preview-bin-hadoop2.7\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SQLContext, SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType\n",
        "from time import sleep\n",
        "from pyspark.streaming import StreamingContext\n",
        "#Spark Contexto\n",
        "#df = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "ss = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark create Streaming example\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()\n",
        "    \n",
        "sc.version "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.0.0-preview'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xx1F6deLBOQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import sleep\n",
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "#sc = SparkContext(master='local[4]')\n",
        "\n",
        "NUM_BATCHES = 10  # a quantidade de números em sequência\n",
        "batches = [sc.parallelize([num]) for num in range(NUM_BATCHES)]\n",
        "\n",
        "BATCH_TIMEOUT = 5 # Tempo limite entre geração de lote\n",
        "ssc = StreamingContext(sc, BATCH_TIMEOUT)\n",
        "dstream = ssc.queueStream(rdds=batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqHAhzs3LBOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "finished = False\n",
        "printed = False\n",
        "\n",
        "def set_ending_flag(rdd):\n",
        "    global finished\n",
        "    if rdd.isEmpty():\n",
        "        finished = True\n",
        "\n",
        "def print_only_at_the_end(rdd):\n",
        "    global printed\n",
        "    if finished and not printed:\n",
        "        print(rdd.collect()[0])\n",
        "        printed = True\n",
        "\n",
        "# Se recebermos o rdd vazio, o fluxo será concluído.\n",
        "# Então imprima o resultado e pare o contexto.\n",
        "\n",
        "dstream.foreachRDD(set_ending_flag)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgjYa0OZLBOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def aggregator(values, old):\n",
        "    return (old or 0) + sum(values)\n",
        "\n",
        "# `updateStateByKey` precisa da estrutura de valores-chave, você precisa especificar a chave fictícia\" res \"\n",
        "# e remova-o após a agregação\n",
        "\n",
        "dstream.map(lambda num: ('res', num))\\\n",
        "    .updateStateByKey(aggregator)\\\n",
        "    .map(lambda x: x[1])\\\n",
        "    .foreachRDD(print_only_at_the_end)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nL0v3yUHLBOj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8fc8d6a8-209e-4f00-b785-0435f892ce18"
      },
      "source": [
        "ssc.checkpoint('./checkpoint')  #  ponto de verificação para armazenar o estado atual\n",
        "ssc.start()\n",
        "while not printed:\n",
        "    pass\n",
        "ssc.stop()\n",
        "sc.stop()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}