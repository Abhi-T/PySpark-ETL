{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "name": "StreamingDemoHemoce.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sukYe_IhLBOL",
        "colab_type": "text"
      },
      "source": [
        "### HEMOCE \n",
        "\n",
        "Dada uma sequência de números ($ 0, 1, 2, ... N $) chegando em tempo real. Você precisa calcular a soma desta sequência.\n",
        "Use uma abordagem sensata para armazenar e atualizar o resultado atual.\n",
        "\n",
        "A saída deve ser um número.\n",
        "\n",
        "**Exemplo**\n",
        "* Sequência de entrada: `0, 1, 2, 3`.\n",
        "* Saída: `6`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rKsPULhHQua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-3.0.0-preview/spark-3.0.0-preview-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.0-preview-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RggOdYsQOA0",
        "colab_type": "code",
        "outputId": "54ad7d26-5665-4404-f28a-3b00ba6e4316",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-preview-bin-hadoop2.7\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SQLContext, SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType\n",
        "from time import sleep\n",
        "from pyspark.streaming import StreamingContext\n",
        "#Spark Contexto\n",
        "#df = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "ss = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark create Streaming example\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()\n",
        "    \n",
        "sc.version "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.0.0-preview'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xx1F6deLBOQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import sleep\n",
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "#sc = SparkContext(master='local[4]')\n",
        "\n",
        "NUM_BATCHES = 4  # a quantidade de números em sequência\n",
        "batches = [sc.parallelize([num]) for num in range(NUM_BATCHES)]\n",
        "\n",
        "BATCH_TIMEOUT = 5 # Tempo limite entre geração de lote\n",
        "ssc = StreamingContext(sc, BATCH_TIMEOUT)\n",
        "dstream = ssc.queueStream(rdds=batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqHAhzs3LBOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "finished = False\n",
        "printed = False\n",
        "\n",
        "def set_ending_flag(rdd): # Seta o final do flag\n",
        "    global finished\n",
        "    if rdd.isEmpty():\n",
        "        finished = True\n",
        "\n",
        "def print_only_at_the_end(rdd): #  imprimir apenas no final\n",
        "    global printed\n",
        "    if finished and not printed:\n",
        "        print(rdd.collect()[0])\n",
        "        printed = True\n",
        "\n",
        "# Se recebermos o rdd vazio, o fluxo será concluído.\n",
        "# Então imprima o resultado e pare o contexto.\n",
        "\n",
        "dstream.foreachRDD(set_ending_flag)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgjYa0OZLBOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def aggregator(values, old):\n",
        "    return (old or 0) + sum(values)\n",
        "\n",
        "# `updateStateByKey` precisa da estrutura de valores-chave, você precisa especificar a chave fictícia\" res \"\n",
        "# e remova-o após a agregação\n",
        "\n",
        "dstream.map(lambda num: ('res', num))\\\n",
        "    .updateStateByKey(aggregator)\\\n",
        "    .map(lambda x: x[1])\\\n",
        "    .foreachRDD(print_only_at_the_end)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nL0v3yUHLBOj",
        "colab_type": "code",
        "outputId": "74279637-7ff9-456d-952f-4e0e0fa50913",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ssc.checkpoint('./checkpoint')  #  ponto de verificação para armazenar o estado atual\n",
        "ssc.start()\n",
        "while not printed:\n",
        "    pass\n",
        "ssc.stop()                      #  Quando ele conta o tempo de NUM_BATCHES ele finaliza\n",
        "sc.stop()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}